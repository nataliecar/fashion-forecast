{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_227/1874384687.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils, models\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cardon/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cardon/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True).to(device)\n",
    "\n",
    "# Freezing the base model layers to prevent retraining\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on the Resnet Model:\n",
    "\n",
    "** convert images to b&w\n",
    "- Requires input images dimensions (256,256) ** resize our images\n",
    "- My additional fully connected layer needs dimensions (2048,10) - 10 for the 10 classes for the 10 style types (--)\n",
    "- Image preprocessing requires:\n",
    "  1. (224,224) center crop\n",
    "  2. image is normalized with mean = 255*[0.485, 0.456, 0.406] and\n",
    "  std = 255*[0.229, 0.224, 0.225]\n",
    "  3. transpose it from HWC to CHW layout\n",
    "- Post-processing involves calculating the softmax probability scores for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classes = 4\n",
    "model.fc = torch.nn.Linear(512, classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss() # multi-class classification model loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 csv_file,      # images could be provided with in a series of directories\n",
    "                 root_dir,     # images could be provided as a list as well\n",
    "                 transform = None):  # provide transformation to apply to each image\n",
    "      \"\"\"\n",
    "      Organize the images and the associated labels into two lists.  Potentially create additional\n",
    "      lists if more complicated information is need.  Important note: images are NOT\n",
    "      read and stored in this initializer.  They are read in __getitem__ as needed.\n",
    "      \"\"\"\n",
    "      self.csv_file = csv_file # path of csv file\n",
    "      self.root_dir = root_dir # directory the photos are in\n",
    "      self.images = pd.read_csv(self.csv_file)\n",
    "      # Record the transform that may need to be applied.\n",
    "      self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Return a tuple with the data, ground truth label, and any other data\n",
    "        associated with a single image.\n",
    "        '''\n",
    "        img_name = self.images.iloc[idx, 0] # name of image in 1st column\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        im = Image.open(img_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "\n",
    "        \"\"\"\n",
    "        label encodes season\n",
    "        season = {\n",
    "            0: 'spring'\n",
    "            1: 'summer'\n",
    "            2: 'fall'\n",
    "            3: 'winter'  \n",
    "        }\n",
    "        \"\"\"\n",
    "        label = self.images.iloc[idx, 1]\n",
    "\n",
    "        return im, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/cardon/Desktop/SEM8/RCOS/fashion-forecast'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224)), \n",
    "                                        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "# transforms.Grayscale(num_output_channels=1)\n",
    "\n",
    "dataset = MyDataset(csv_file='./filtered_style_stats.csv',\n",
    "                    root_dir='./yolov5/yolov5/crop-images',\n",
    "                    transform=image_transforms)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.7, 0.15, 0.15], generator=torch.Generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced from \n",
    "# https://towardsdatascience.com/pytorch-basics-sampling-samplers-2a0f29f0bf2a\n",
    "\n",
    "def get_class_distribution(dataset_obj):\n",
    "    count_dict = {          \\\n",
    "        'spring': 0,        \\\n",
    "        'summer': 0,        \\\n",
    "        'fall':   0,        \\\n",
    "        'winter': 0         \\\n",
    "    } # type: ignore\n",
    "    idx_to_class = {        \\\n",
    "        0: 'spring',        \\\n",
    "        1: 'summer',        \\\n",
    "        2: 'fall',          \\\n",
    "        3: 'winter'         \\\n",
    "    }\n",
    "    \n",
    "    for idx in range(len(dataset_obj)):\n",
    "        element = dataset_obj[idx]\n",
    "        y_lbl = idx_to_class[element[1]]\n",
    "        count_dict[y_lbl] += 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of total data: 267964\n",
      "Length of train_dataset: 187575; 70.00%\n",
      "Length of val_dataset: 40195; 15.00%\n",
      "Length of test_dataset: 40194; 15.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### CHECKING ITS DECENT ####\n",
    "total_data = len(train_dataset) + len(val_dataset) + len(test_dataset)\n",
    "print(f\"Length of total data: {total_data}\")\n",
    "print(f\"Length of train_dataset: {len(train_dataset)}; {(len(train_dataset)/total_data)*100:.2f}%\")\n",
    "print(f\"Length of val_dataset: {len(val_dataset)}; {(len(val_dataset)/total_data)*100:.2f}%\")\n",
    "print(f\"Length of test_dataset: {len(test_dataset)}; {(len(test_dataset)/total_data)*100:.2f}%\\n\")\n",
    "#print(f\"Class Distribution of train_dataset: {get_class_distribution(train_dataset)}\")\n",
    "#print(f\"Class Distribution of val_dataset: {get_class_distribution(val_dataset)}\")\n",
    "#print(f\"Class Distribution of test_dataset: {get_class_distribution(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(train_dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 800 == 0: # every 20000 images run\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, incorrect_examples, correct_examples):\n",
    "    size = len(val_dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad(): \n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            val = pred.argmax(1).to(device) \n",
    "            correct += ((val == y).type(torch.float).sum().item()) \n",
    "\n",
    "            if torch.all(torch.eq(val, y)) and len(correct_examples) < 6:\n",
    "                correct_examples.append(X.cpu())\n",
    "            if (not torch.all(torch.eq(val, y))) and len(incorrect_examples) < 6:\n",
    "                incorrect_examples.append(X.cpu())\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Val Error ---\\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n",
    "    return correct_examples, incorrect_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.420578  [    0/187575]\n",
      "loss: 1.382192  [25600/187575]\n",
      "loss: 1.259995  [51200/187575]\n",
      "loss: 1.219778  [76800/187575]\n",
      "loss: 1.327443  [102400/187575]\n",
      "loss: 1.236579  [128000/187575]\n",
      "loss: 1.517246  [153600/187575]\n",
      "loss: 1.329134  [179200/187575]\n",
      "Val Error ---\n",
      " Accuracy: 41.0%, Avg loss: 1.252456 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.158244  [    0/187575]\n",
      "loss: 1.375487  [25600/187575]\n",
      "loss: 1.246830  [51200/187575]\n",
      "loss: 1.216655  [76800/187575]\n",
      "loss: 1.329320  [102400/187575]\n",
      "loss: 1.233394  [128000/187575]\n",
      "loss: 1.516169  [153600/187575]\n",
      "loss: 1.337231  [179200/187575]\n",
      "Val Error ---\n",
      " Accuracy: 41.2%, Avg loss: 1.252164 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.157117  [    0/187575]\n",
      "loss: 1.373440  [25600/187575]\n",
      "loss: 1.241705  [51200/187575]\n",
      "loss: 1.218745  [76800/187575]\n",
      "loss: 1.331375  [102400/187575]\n",
      "loss: 1.234507  [128000/187575]\n",
      "loss: 1.511586  [153600/187575]\n",
      "loss: 1.338363  [179200/187575]\n",
      "Val Error ---\n",
      " Accuracy: 41.1%, Avg loss: 1.252319 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.158041  [    0/187575]\n",
      "loss: 1.371028  [25600/187575]\n",
      "loss: 1.239654  [51200/187575]\n",
      "loss: 1.219787  [76800/187575]\n",
      "loss: 1.332161  [102400/187575]\n",
      "loss: 1.236308  [128000/187575]\n",
      "loss: 1.507983  [153600/187575]\n",
      "loss: 1.337844  [179200/187575]\n",
      "Val Error ---\n",
      " Accuracy: 41.2%, Avg loss: 1.252502 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.158892  [    0/187575]\n",
      "loss: 1.368902  [25600/187575]\n",
      "loss: 1.238713  [51200/187575]\n",
      "loss: 1.220343  [76800/187575]\n",
      "loss: 1.332541  [102400/187575]\n",
      "loss: 1.237955  [128000/187575]\n",
      "loss: 1.505376  [153600/187575]\n",
      "loss: 1.337112  [179200/187575]\n",
      "Val Error ---\n",
      " Accuracy: 41.2%, Avg loss: 1.252650 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.159459  [    0/187575]\n",
      "loss: 1.367168  [25600/187575]\n",
      "loss: 1.238240  [51200/187575]\n",
      "loss: 1.220693  [76800/187575]\n",
      "loss: 1.332740  [102400/187575]\n",
      "loss: 1.239268  [128000/187575]\n",
      "loss: 1.503469  [153600/187575]\n",
      "loss: 1.336490  [179200/187575]\n",
      "Val Error ---\n",
      " Accuracy: 41.2%, Avg loss: 1.252760 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.159804  [    0/187575]\n",
      "loss: 1.365795  [25600/187575]\n",
      "loss: 1.237996  [51200/187575]\n",
      "loss: 1.220936  [76800/187575]\n",
      "loss: 1.332835  [102400/187575]\n",
      "loss: 1.240263  [128000/187575]\n",
      "loss: 1.502034  [153600/187575]\n",
      "loss: 1.336011  [179200/187575]\n",
      "Val Error ---\n",
      " Accuracy: 41.2%, Avg loss: 1.252838 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.160002  [    0/187575]\n",
      "loss: 1.364723  [25600/187575]\n",
      "loss: 1.237880  [51200/187575]\n",
      "loss: 1.221112  [76800/187575]\n",
      "loss: 1.332866  [102400/187575]\n",
      "loss: 1.241000  [128000/187575]\n",
      "loss: 1.500921  [153600/187575]\n",
      "loss: 1.335650  [179200/187575]\n",
      "Val Error ---\n",
      " Accuracy: 41.2%, Avg loss: 1.252892 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.160109  [    0/187575]\n",
      "loss: 1.363894  [25600/187575]\n",
      "loss: 1.237840  [51200/187575]\n",
      "loss: 1.221244  [76800/187575]\n",
      "loss: 1.332856  [102400/187575]\n",
      "loss: 1.241541  [128000/187575]\n",
      "loss: 1.500030  [153600/187575]\n",
      "loss: 1.335370  [179200/187575]\n",
      "Val Error ---\n",
      " Accuracy: 41.1%, Avg loss: 1.252928 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.160160  [    0/187575]\n",
      "loss: 1.363257  [25600/187575]\n",
      "loss: 1.237848  [51200/187575]\n",
      "loss: 1.221344  [76800/187575]\n",
      "loss: 1.332822  [102400/187575]\n",
      "loss: 1.241939  [128000/187575]\n",
      "loss: 1.499294  [153600/187575]\n",
      "loss: 1.335147  [179200/187575]\n",
      "Val Error ---\n",
      " Accuracy: 41.1%, Avg loss: 1.252951 \n",
      "\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    incorrect_examples = []\n",
    "    correct_examples = []\n",
    "    correct_examples, incorrect_examples = test(val_dataloader, model, loss_fn, incorrect_examples, correct_examples)\n",
    "print(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_examples, incorrect_examples = test(test_dataloader, model, loss_fn, incorrect_examples, correct_examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
