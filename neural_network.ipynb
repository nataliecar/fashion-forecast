{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /home/cardon/miniconda3/lib/python3.11/site-packages (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_237/2872050913.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils, models\n",
    "%pip install torchsummary\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cardon/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cardon/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True).to(device)\n",
    "\n",
    "# Freezing the base model layers to prevent retraining\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on the Resnet Model:\n",
    "\n",
    "** convert images to b&w\n",
    "- Requires input images dimensions (256,256) ** resize our images\n",
    "- My additional fully connected layer needs dimensions (2048,10) - 10 for the 10 classes for the 10 style types (--)\n",
    "- Image preprocessing requires:\n",
    "  1. (224,224) center crop\n",
    "  2. image is normalized with mean = 255*[0.485, 0.456, 0.406] and\n",
    "  std = 255*[0.229, 0.224, 0.225]\n",
    "  3. transpose it from HWC to CHW layout\n",
    "- Post-processing involves calculating the softmax probability scores for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classes = 4\n",
    "model.fc = torch.nn.Linear(512, classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss() # multi-class classification model loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 csv_file,      # images could be provided with in a series of directories\n",
    "                 root_dir,     # images could be provided as a list as well\n",
    "                 transform = transforms.ToTensor()):  # provide transformation to apply to each image\n",
    "      \"\"\"\n",
    "      Organize the images and the associated labels into two lists.  Potentially create additional\n",
    "      lists if more complicated information is need.  Important note: images are NOT\n",
    "      read and stored in this initializer.  They are read in __getitem__ as needed.\n",
    "      \"\"\"\n",
    "      self.csv_file = csv_file # path of csv file\n",
    "      self.root_dir = root_dir # directory the photos are in\n",
    "      self.images = pd.read_csv(self.csv_file)\n",
    "      # Record the transform that may need to be applied.\n",
    "      self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Return a tuple with the data, ground truth label, and any other data\n",
    "        associated with a single image.\n",
    "        '''\n",
    "        img_name = self.images.iloc[idx, 0] # name of image in 1st column\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        im = Image.open(img_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "\n",
    "        \"\"\"\n",
    "        label encodes season\n",
    "        season = {\n",
    "            0: 'spring'\n",
    "            1: 'summer'\n",
    "            2: 'fall'\n",
    "            3: 'winter'  \n",
    "        }\n",
    "        \"\"\"\n",
    "        label = self.images.iloc[idx, 1]\n",
    "\n",
    "        return im, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/cardon/Desktop/SEM8/RCOS/fashion-forecast'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224)), \\\n",
    "                                       transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "# transforms.Grayscale(num_output_channels=1)\n",
    "\n",
    "dataset = MyDataset(csv_file='./filtered_style_stats.csv',\n",
    "                    root_dir='./yolov5/yolov5/runs/detect/exp4',\n",
    "                    transform=image_transforms)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.7, 0.15, 0.15], generator=torch.Generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced from \n",
    "# https://towardsdatascience.com/pytorch-basics-sampling-samplers-2a0f29f0bf2a\n",
    "\n",
    "def get_class_distribution(dataset_obj):\n",
    "    count_dict = {          \\\n",
    "        'spring': 0,        \\\n",
    "        'summer': 0,        \\\n",
    "        'fall':   0,        \\\n",
    "        'winter': 0         \\\n",
    "    } # type: ignore\n",
    "    idx_to_class = {        \\\n",
    "        0: 'spring',        \\\n",
    "        1: 'summer',        \\\n",
    "        2: 'fall',          \\\n",
    "        3: 'winter'         \\\n",
    "    }\n",
    "    \n",
    "    for idx in range(len(dataset_obj)):\n",
    "        element = dataset_obj[idx]\n",
    "        y_lbl = idx_to_class[element[1]]\n",
    "        count_dict[y_lbl] += 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of total data: 279203\n",
      "Length of train_dataset: 195443; 70.00%\n",
      "Length of val_dataset: 41880; 15.00%\n",
      "Length of test_dataset: 41880; 15.00%\n",
      "\n",
      "Class Distribution of train_dataset: {'spring': 48684, 'summer': 74338, 'fall': 48271, 'winter': 24150}\n",
      "Class Distribution of val_dataset: {'spring': 10465, 'summer': 16065, 'fall': 10238, 'winter': 5112}\n",
      "Class Distribution of test_dataset: {'spring': 10445, 'summer': 15974, 'fall': 10255, 'winter': 5206}\n"
     ]
    }
   ],
   "source": [
    "#### CHECKING ITS DECENT ####\n",
    "total_data = len(train_dataset) + len(val_dataset) + len(test_dataset)\n",
    "print(f\"Length of total data: {total_data}\")\n",
    "print(f\"Length of train_dataset: {len(train_dataset)}; {(len(train_dataset)/total_data)*100:.2f}%\")\n",
    "print(f\"Length of val_dataset: {len(val_dataset)}; {(len(val_dataset)/total_data)*100:.2f}%\")\n",
    "print(f\"Length of test_dataset: {len(test_dataset)}; {(len(test_dataset)/total_data)*100:.2f}%\\n\")\n",
    "print(f\"Class Distribution of train_dataset: {get_class_distribution(train_dataset)}\")\n",
    "print(f\"Class Distribution of val_dataset: {get_class_distribution(val_dataset)}\")\n",
    "print(f\"Class Distribution of test_dataset: {get_class_distribution(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(train_dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, incorrect_examples, correct_examples):\n",
    "    size = len(val_dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad(): \n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            val = pred.argmax(1).to(device) \n",
    "            correct += ((val == y).type(torch.float).sum().item()) \n",
    "\n",
    "            if torch.all(torch.eq(val, y)) and len(correct_examples) < 6:\n",
    "                correct_examples.append(X.cpu())\n",
    "            if (not torch.all(torch.eq(val, y))) and len(incorrect_examples) < 6:\n",
    "                incorrect_examples.append(X.cpu())\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Val Error ---\\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n",
    "    return correct_examples, incorrect_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.147855  [    0/195443]\n",
      "loss: 1.199678  [ 3200/195443]\n",
      "loss: 1.248353  [ 6400/195443]\n",
      "loss: 1.469307  [ 9600/195443]\n",
      "loss: 1.019715  [12800/195443]\n",
      "loss: 1.306609  [16000/195443]\n",
      "loss: 1.313135  [19200/195443]\n",
      "loss: 1.255340  [22400/195443]\n",
      "loss: 1.152336  [25600/195443]\n",
      "loss: 1.224064  [28800/195443]\n",
      "loss: 1.145435  [32000/195443]\n",
      "loss: 1.210174  [35200/195443]\n",
      "loss: 1.249001  [38400/195443]\n",
      "loss: 1.216153  [41600/195443]\n",
      "loss: 1.155298  [44800/195443]\n",
      "loss: 1.125935  [48000/195443]\n",
      "loss: 1.369823  [51200/195443]\n",
      "loss: 1.288450  [54400/195443]\n",
      "loss: 1.233435  [57600/195443]\n",
      "loss: 1.301964  [60800/195443]\n",
      "loss: 1.354385  [64000/195443]\n",
      "loss: 1.231669  [67200/195443]\n",
      "loss: 1.364170  [70400/195443]\n",
      "loss: 1.146530  [73600/195443]\n",
      "loss: 1.391031  [76800/195443]\n",
      "loss: 1.095357  [80000/195443]\n",
      "loss: 1.238365  [83200/195443]\n",
      "loss: 1.235180  [86400/195443]\n",
      "loss: 1.259393  [89600/195443]\n",
      "loss: 1.302628  [92800/195443]\n",
      "loss: 1.222837  [96000/195443]\n",
      "loss: 1.250922  [99200/195443]\n",
      "loss: 1.158262  [102400/195443]\n",
      "loss: 1.172479  [105600/195443]\n",
      "loss: 1.199838  [108800/195443]\n",
      "loss: 1.148190  [112000/195443]\n",
      "loss: 1.441991  [115200/195443]\n",
      "loss: 1.252242  [118400/195443]\n",
      "loss: 1.199666  [121600/195443]\n",
      "loss: 1.322260  [124800/195443]\n",
      "loss: 1.272076  [128000/195443]\n",
      "loss: 1.139237  [131200/195443]\n",
      "loss: 1.201037  [134400/195443]\n",
      "loss: 1.426203  [137600/195443]\n",
      "loss: 1.128825  [140800/195443]\n",
      "loss: 1.180543  [144000/195443]\n",
      "loss: 1.092348  [147200/195443]\n",
      "loss: 1.301467  [150400/195443]\n",
      "loss: 1.274021  [153600/195443]\n",
      "loss: 1.221147  [156800/195443]\n",
      "loss: 1.377343  [160000/195443]\n",
      "loss: 1.360430  [163200/195443]\n",
      "loss: 1.363503  [166400/195443]\n",
      "loss: 1.216124  [169600/195443]\n",
      "loss: 1.314373  [172800/195443]\n",
      "loss: 1.132914  [176000/195443]\n",
      "loss: 1.297588  [179200/195443]\n",
      "loss: 1.196776  [182400/195443]\n",
      "loss: 1.215971  [185600/195443]\n",
      "loss: 1.226049  [188800/195443]\n",
      "loss: 1.416225  [192000/195443]\n",
      "loss: 1.226997  [195200/195443]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'val_loss' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     incorrect_examples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m     correct_examples \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m     correct_examples, incorrect_examples \u001b[38;5;241m=\u001b[39m test(val_dataloader, model, loss_fn, incorrect_examples, correct_examples)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[62], line 20\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(dataloader, model, loss_fn, incorrect_examples, correct_examples)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(torch\u001b[38;5;241m.\u001b[39meq(val, y))) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(incorrect_examples) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m6\u001b[39m:\n\u001b[1;32m     18\u001b[0m             incorrect_examples\u001b[38;5;241m.\u001b[39mappend(X\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m---> 20\u001b[0m val_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m num_batches\n\u001b[1;32m     21\u001b[0m correct \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Error ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mcorrect)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>0.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Avg loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'val_loss' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    incorrect_examples = []\n",
    "    correct_examples = []\n",
    "    correct_examples, incorrect_examples = test(val_dataloader, model, loss_fn, incorrect_examples, correct_examples)\n",
    "print(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_examples = []\n",
    "correct_examples = []\n",
    "correct_examples, incorrect_examples = test(val_dataloader, model, loss_fn, incorrect_examples, correct_examples)\n",
    "print(\"Done!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
