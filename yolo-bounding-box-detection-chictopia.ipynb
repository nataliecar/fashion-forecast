{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nLink to Documentation: https://github.com/ultralytics/yolov5\\n\\nRUN IN TERMINAL BEFORE PROCEEDED IN THE 'fashion-forecast' DIRECTORY:\\n>   pip install ultralytics\\n>   git clone https://github.com/ultralytics/yolov5  \\n>   cd yolov5\\n>   pip install -r requirements.txt  \\n>   cd ..\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Link to Documentation: https://github.com/ultralytics/yolov5\n",
    "\n",
    "RUN IN TERMINAL BEFORE PROCEEDED IN THE 'fashion-forecast' DIRECTORY:\n",
    ">   pip install ultralytics\n",
    ">   git clone https://github.com/ultralytics/yolov5  \n",
    ">   cd yolov5\n",
    ">   pip install -r requirements.txt  \n",
    ">   cd ..\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\cardon/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-2-16 Python-3.8.5 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True) #force_reload=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCODE TO CHANGE DEVICE:\\nmodel.cpu()  # CPU\\nmodel.cuda()  # GPU\\nmodel.to(device)  # i.e. device=torch.device(0)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "CODE TO CHANGE DEVICE:\n",
    "model.cpu()  # CPU\n",
    "model.cuda()  # GPU\n",
    "model.to(device)  # i.e. device=torch.device(0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running... 0.00% finished\n",
      "Running... 0.36% finished\n",
      "Running... 0.72% finished\n",
      "Running... 1.07% finished\n",
      "Running... 1.43% finished\n",
      "Running... 1.79% finished\n",
      "Running... 2.15% finished\n",
      "Running... 2.51% finished\n",
      "Running... 2.87% finished\n",
      "Running... 3.22% finished\n",
      "Running... 3.58% finished\n",
      "Running... 3.94% finished\n",
      "Running... 4.30% finished\n",
      "Running... 4.66% finished\n",
      "Running... 5.01% finished\n",
      "Running... 5.37% finished\n",
      "Running... 5.73% finished\n",
      "Running... 6.09% finished\n",
      "Running... 6.45% finished\n",
      "Running... 6.81% finished\n",
      "Running... 7.16% finished\n",
      "Running... 7.52% finished\n",
      "Running... 7.88% finished\n",
      "Running... 8.24% finished\n",
      "Running... 8.60% finished\n",
      "Running... 8.95% finished\n",
      "Running... 9.31% finished\n",
      "Running... 9.67% finished\n",
      "Running... 10.03% finished\n",
      "Running... 10.39% finished\n",
      "Running... 10.74% finished\n",
      "Running... 11.10% finished\n",
      "Running... 11.46% finished\n",
      "Running... 11.82% finished\n",
      "Running... 12.18% finished\n",
      "Running... 12.54% finished\n",
      "Running... 12.89% finished\n",
      "Running... 13.25% finished\n",
      "Running... 13.61% finished\n",
      "Running... 13.97% finished\n",
      "Running... 14.33% finished\n",
      "Running... 14.68% finished\n",
      "Running... 15.04% finished\n",
      "Running... 15.40% finished\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 703. KiB for an array with shape (600, 400, 3) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d4cbef5290fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mimg_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mpil_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mnp_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpil_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;31m#plt.imshow(np_img)             # used to display the images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#plt.show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 703. KiB for an array with shape (600, 400, 3) and data type uint8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279203\n",
      "Running... 0.00% finished\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "img_names = []\n",
    "directory = '../unpack-data/'\n",
    "printout_size = 10000\n",
    "seasons_only_csv = pd.read_csv('filtered_style_stats.csv')\n",
    "# Only has memory for 40,000 images at a time\n",
    "counter = 0\n",
    "for img_name in os.listdir(directory):\n",
    "    if counter == len(seasons_only_csv.index):\n",
    "        print(\"Breaking the loop at \", counter)\n",
    "        break\n",
    "    if img_name not in seasons_only_csv.iloc[:, 0].values:\n",
    "        continue\n",
    "    if counter%printout_size == 0:\n",
    "        print(f\"Running... {(counter/len(seasons_only_csv.index))*100:.2f}% finished\")\n",
    "    img_path = os.path.join(directory, img_name)\n",
    "    pil_img = Image.open(img_path)\n",
    "    np_img = np.array(pil_img)\n",
    "    #plt.imshow(np_img)             # used to display the images\n",
    "    #plt.show()\n",
    "    img_list.append(np_img)\n",
    "    img_names.append(str(img_path))\n",
    "    counter += 1\n",
    "print(f'Finished with a set of {len(img_list)} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saved 364 images to \u001b[1mruns\\detect\\exp5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model(img_list)  # inference\n",
    "results.save()   # cropped detections dictionary\n",
    "#results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor idx, df in enumerate(detection_df):\\n    person_df = df[df['name']=='person']\\n    print(person_df.index)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_df = results.pandas().xyxy\n",
    "print(len(detection_df))\n",
    "'''\n",
    "for idx, df in enumerate(detection_df):\n",
    "    person_df = df[df['name']=='person']\n",
    "    print(person_df.index)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Takes the current results and saves only the information from the maximum 'person' detections\"\"\"\n",
    "\n",
    "cleaned_df_lst = []\n",
    "for idx, df in enumerate(detection_df):\n",
    "    person_df = df[df['name']=='person']\n",
    "    if person_df.empty == True:\n",
    "        continue\n",
    "    if (len(person_df.index) == 1):\n",
    "        row = person_df.iloc[0].tolist()\n",
    "        row.append(idx)\n",
    "        cleaned_df_lst.append(row)\n",
    "        continue\n",
    "    # multiple detections -> choose the highest confidence of person\n",
    "    maxValueIndex = person_df['confidence'].idxmax()\n",
    "    row = person_df.iloc[maxValueIndex].tolist()\n",
    "    row.append(idx)\n",
    "    cleaned_df_lst.append(row)\n",
    "\n",
    "col_names = list(detection_df[0].columns) + ['df_idx']\n",
    "cleaned_df = pd.DataFrame(cleaned_df_lst, columns = col_names)\n",
    "TOTAL_IMG = len(cleaned_df.index)\n",
    "print(TOTAL_IMG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to crop image in a 1:1 aspect ratio no smaller than 256 pixels centered around the person detection\n",
    "def crop_image(img, xmin, ymin, xmax, ymax): \n",
    "    max_dim = max((xmax-xmin), (ymax-ymin))\n",
    "    img = img[ymin:ymax, xmin:xmax]\n",
    "    img_square = np.ones([max_dim+1,max_dim+1], dtype=np.uint8)*127\n",
    "    xmin, xmax, ymin, ymax = 0, (xmax-xmin), 0, (ymax-ymin)\n",
    "    img_square[ymin:ymax, xmin:xmax] = img\n",
    "    return img_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Takes the cleaned df data and crops each image to it's detected bounding box dimensions\"\"\"\n",
    "directory = '../unpack-data/'\n",
    "save_dir = '../crop-data/'\n",
    "df_idx = 0\n",
    "for idx, img_name in enumerate(os.listdir(directory)):\n",
    "    if idx == TOTAL_IMG:\n",
    "        break\n",
    "    if idx not in cleaned_df['df_idx'].values:\n",
    "        continue\n",
    "\n",
    "    xmin, ymin, xmax, ymax = cleaned_df.loc[df_idx][['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "    #print(int(xmin), int(ymin), int(xmax), int(ymax))\n",
    "    df_idx += 1\n",
    "    img_path = os.path.join(directory, img_name)\n",
    "    pil_img = Image.open(img_path).convert(\"L\")\n",
    "    np_img = np.array(pil_img)\n",
    "    crop_img = crop_image(np_img, int(xmin), int(ymin), int(xmax), int(ymax))\n",
    "    #plt.imshow(crop_img, cmap='gray')\n",
    "    save_path = os.path.join(save_dir, (img_name+'.jpg'))\n",
    "    save_im = Image.fromarray(crop_img)\n",
    "    save_im.save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
